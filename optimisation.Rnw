\chapter{Fonctions d'optimisation}
\label{optimisation}

<<echo=FALSE>>=
options(width=52)
@

\begin{objectifs}
\item Connaître et savoir utiliser les différentes fonctions de calcul
  de racines de R.
\item Connaître et savoir utiliser les différentes fonctions
  d'optimisation de R.
\item Savoir reformuler un problème d'optimisation en base
  logarithmique pour éviter les difficultés numériques.
\end{objectifs}

Les méthodes de bissection, du point fixe, de Newton--Raphson et
consorts permettent de résoudre des équations à une variable de la
forme $f(x) = 0$ ou $g(x) = x$. Il existe également des versions de
ces méthodes pour les systèmes à plusieurs variables de la forme
\begin{align*}
  f_1(x_1, x_2, x_3) &= 0 \\
  f_2(x_1, x_2, x_3) &= 0 \\
  f_3(x_1, x_2, x_3) &= 0.
\end{align*}

De tels systèmes d'équations surviennent plus souvent qu'autrement
lors de l'optimisation d'une fonction. Par exemple, en recherchant le
maximum ou le minimum d'une fonction $f(x, y)$, on souhaitera résoudre
le système d'équations
\begin{align*}
  \frac{\partial}{\partial x}\, f(x, y) &= 0 \\
  \frac{\partial}{\partial y}\, f(x, y) &= 0.
\end{align*}

En inférence statistique, les fonctions d'optimisation sont
fréquemment employées pour calculer numériquement des estimateurs du
maximum de vraisemblance.

La grande majorité des suites logicielles de calcul comportent des
outils d'optimisation de fonctions. Ce chapitre passe en revue les
fonctions disponibles dans R. Comme pour les chapitres précédents, des
exemples d'utilisation de chacune des fonctions se trouvent dans le
code informatique de la \autoref{optimisation:exemples}.


\section{Fonctions d'optimisation et de calcul de racines}
\label{optimisation:fonctions}

Le système R compte un certain nombre de fonctions pour trouver
numériquement le minimum ou le maximum d'une fonction ainsi que pour
calculer la racine d'une fonction dans un intervalle ou toutes les
racines d'un polynôme. Ces fonctions diffèrent par certaines de leurs
fonctionnalités et leur interface, mais aussi par les algorithmes
utilisés. Consulter les rubriques d'aide pour les détails.

%% Le contenu de cette section est conservé dans un fichier à part
%% pour pouvoir être inclus dans les notes du cours de méthodes
%% numériques.
\input{optimisation-fonctions}


\section{Astuce Ripley}
\label{sec:resolution:astuce}

Brian Ripley --- un important développeur de R --- a publié le truc
suivant dans les forums de discussion de R. Puisqu'il est très utile,
nous nous permettons de le disséminer.

Une application statistique fréquente de l'optimisation est la
maximisation numérique d'une fonction de vraisemblance ou, plus
communément, la minimisation de la log-vraisemblance négative
\begin{equation*}
  -l(\theta) = - \sum_{i = 1}^n \ln f(x_i; \theta).
\end{equation*}
Les fonctions d'optimisation sont d'ailleurs illustrées dans ce
contexte dans le code informatique de la
\autoref{optimisation:exemples}.

Plusieurs lois de probabilité ont des paramètres strictement positifs.
Or, en pratique, il n'est pas rare que les fonctions d'optimisation
s'égarent dans les valeurs négatives des paramètres. La fonction de
densité n'étant pas définie, la log-vraisemblance vaut alors
\code{NaN} et cela peut faire complètement dérailler la procédure
d'optimisation ou, à tout le moins, susciter des doutes sur la
validité de la réponse.

Afin de pallier à ce problème, l'Astuce Ripley{\texttrademark} propose
d'estimer non pas les paramètres de la loi eux-mêmes, mais plutôt
leurs logarithmes. Si l'on définit $\tilde{\theta} = \ln \theta$,
alors on peut écrire la fonction de log-vraisemblance ci-dessus sous
la forme
\begin{equation*}
  -l(\tilde{\theta}) = - \sum_{i = 1}^n \ln f(x_i; e^{\tilde{\theta}}).
\end{equation*}
Dès lors, $\tilde{\theta}$ (qui peut représenter un ou plusieurs
paramètres) demeure valide sur tout l'axe des réels, ce qui permet
d'éviter bien des soucis de nature numérique lors de la minimisation
de $-l(\tilde{\theta})$.

Évidemment, le résultat de l'optimisation est l'estimateur du maximum
de vraisemblance de $\tilde{\theta}$. Il faudra donc veiller à faire
la transformation inverse pour retrouver l'estimateur de $\theta$.

L'utilisation de l'astuce est illustrée à la
\autoref{optimisation:exemples}.


\section{Pour en savoir plus}
\label{optimisation:plus}

Les packages disponible sur CRAN fournissent plusieurs autres outils
d'optimisation pour R. Pour un bon résumé des options disponibles,
consulter la \emph{CRAN Task View} consacrée à l'optimisation:
\begin{quote}
  \url{http://cran.r-project.org/web/views/Optimization.html}
\end{quote}


\section{Exemples}
\label{optimisation:exemples}

\lstinputlisting[firstline=3]{optimisation.R}



\section{Exercices}
\label{optimisation:exercices}

\Opensolutionfile{solutions}[solutions-optimisation]

\begin{Filesave}{solutions}
\section*{Chapitre \ref*{optimisation}}
\addcontentsline{toc}{section}{Chapitre \protect\ref*{optimisation}}

\end{Filesave}

\begin{exercice}
  Trouver la solution des équations suivantes à l'aide des fonctions R
  appropriées.
  \begin{enumerate}
  \item $x^3 - 2 x^2 - 5 = 0$ pour $1 \leq x \leq 4$
  \item $x^3 + 3 x^2 - 1 = 0$ pour $-4 \leq x \leq 0$
  \item $x - 2^{-x} = 0$ pour $0 \leq x \leq 1$
  \item $e^x + 2^{-x} + 2 \cos x - 6 = 0$ pour $1 \leq x \leq 2$
  \item $e^x - x^2 + 3x - 2 = 0$ pour $0 \leq x \leq 1$
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
\item
<<echo=TRUE,eval=FALSE>>=
f <- function(x) x^3 - 2 * x^2 - 5
uniroot(f, lower = 1, upper = 4)
@
\item Comme un simple graphique le démontre, il y a deux racines dans
  l'intervalle.
<<echo=TRUE,eval=FALSE>>=
f <- function(x) x^3 + 3 * x^2 - 1
curve(f, xlim = c(-4, 0))
uniroot(f, lower = -4, upper = -1)
uniroot(f, lower = -1, upper = 0)
@
\item
<<echo=TRUE,eval=FALSE>>=
f <- function(x) x - 2^(-x)
uniroot(f, lower = 0, upper = 1)
@
\item
<<echo=TRUE,eval=FALSE>>=
f <- function(x) exp(x) + 2^(-x) + 2 * cos(x) - 6
uniroot(f, lower = 1, upper = 2)
@
\item
<<echo=TRUE,eval=FALSE>>=
f <- function(x) exp(x) - x^2 + 3 * x - 2
uniroot(f, lower = 0, upper = 1)
@
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  En théorie de la crédibilité, l'estimateur d'un paramètre $a$ est
  donné sous forme de point fixe
  \begin{displaymath}
    \hat{a} = \frac{1}{n - 1} \sum_{i=1}^n z_i (X_i - \bar{X}_z)^2,
  \end{displaymath}
  où
  \begin{align*}
    z_i &= \frac{\hat{a} w_i}{\hat{a} w_i + s^2} \\
    \bar{X}_z &= \sum_{i=1}^n \frac{z_i}{z_\pt} X_i
  \end{align*}
  et $X_1, \dots, X_n$, $w_1, \dots, w_n$ et $s^2$ sont des données.
  Calculer la valeur de $\hat{a}$ si $s^2 = \nombre{140 000 000}$ et
  que les valeurs de $X_i$ et $w_i$ sont telles qu'elles apparaissent
  dans le tableau ci-dessous.
  \begin{center}
    \begin{tabular}{crrrrr}
      \toprule
      $i$ & 1 & 2 & 3 & 4 & 5 \\
      \midrule
      $X_i$ &
      \nombre{2061} & \nombre{1511} &
      \nombre{1806} & \nombre{1353} & \nombre{1600} \\
      $w_i$ &
      \nombre{100155} & \nombre{19895} &
      \nombre{13735}  & \nombre{4152}  & \nombre{36110} \\
      \bottomrule
    \end{tabular}
  \end{center}
  \begin{sol}
<<echo=TRUE,eval=FALSE>>=
X <- c(2061, 1511, 1806, 1353, 1600)
w <- c(100155, 19895, 13735, 4152, 36110)
g <- function(a, X, w, s2)
{
    z <- 1/(1 + s2/(a * w))
    Xz <- sum(z * X)/sum(z)
    sum(z * (X - Xz)^2)/(length(X) - 1)
}
uniroot(function(x) g(x, X, w, 140E6) - x, c(50000, 80000))
@
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{exercice:optimisation:optim}
  Les fonctions de densité de probabilité et de répartition de la
  distribution de Pareto sont données à
  l'\autoref{exercice:avance:pareto}. Calculer les estimateurs du
  maximum de vraisemblance des paramètres de la Pareto à partir d'un
  échantillon aléatoire obtenu par simulation avec la commande
<<echo=TRUE,eval=FALSE>>=
x <- lambda * (runif(100)^(-1/alpha) - 1)
@
  pour des valeurs de \code{alpha} et \code{lambda} choisies.
  \begin{sol}
<<echo=TRUE,eval=FALSE>>=
dpareto <- function(x, alpha, lambda)
{
    (alpha * lambda^alpha)/(x + lambda)^(alpha+1)
}
f <- function(par, x) -sum(log(dpareto(x, par[1], par[2])))
optim(c(1, 1000), f, x = x)
@
    ou, en utilisant le truc du logarithme des paramètres expliqué
    dans le code informatique de la \autoref{optimisation:exemples}
    pour éviter les soucis de convergence:
<<echo=TRUE,eval=FALSE>>=
dpareto <- function(x, logAlpha, logLambda)
{
    alpha <- exp(logAlpha)
    lambda <- exp(logLambda)
    (alpha * lambda^alpha)/(x + lambda)^(alpha+1)
}
optim(c(log(2), log(1000)), f, x = x)
exp(optim(c(log(2), log(1000)), f, x = x)$par)
@
  \end{sol}
\end{exercice}

\Closesolutionfile{solutions}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "introduction_programmation_r"
%%% coding: utf-8
%%% End:
